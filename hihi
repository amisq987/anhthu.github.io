```python
import tensorflow as tf
import numpy as np
from datetime import datetime
import pandas as pd
from sklearn.metrics import mean_absolute_error, r2_score

# Assuming the following functions are defined as in the XGBoost code
# def smape(y_true, y_pred): ...
# def nmae(y_true, y_pred): ...
# def duan_smearing_factor(y_log_true, y_log_pred): ...
# def inverse_log1p_with_smearing(y_log_pred, smear_factor=None): ...

# Assuming evaluate function is defined elsewhere, e.g.:
# def evaluate(y_true, y_pred):
#     return {
#         'MAE': mean_absolute_error(y_true, y_pred),
#         'R2': r2_score(y_true, y_pred),
#         'sMAPE': smape(y_true, y_pred)
#     }

# Assuming resultsDict and predictionsDict are defined

# Parameters
BATCH_SIZE = 64
BUFFER_SIZE = 1000  # Increased for better shuffling
WINDOW_LENGTH = 24

# Use the same window_data function
def window_data(X, Y, window=7):
    '''
    The dataset length will be reduced to guarantee all samples have the window, so new length will be len(dataset)-window+1
    '''
    x = []
    y = []
    for i in range(window-1, len(X)):
        x.append(X[i-window+1:i+1])
        y.append(Y[i])
    return np.array(x), np.array(y)

# Use log-transformed target to align with XGBoost
y_train = np.log1p(df_train['APE_Sub'])
y_test = np.log1p(df_test['APE_Sub'])

# X_train and X_test are already defined as DataFrames in XGBoost code, convert to numpy
X_train_np = X_train.to_numpy()
X_test_np = X_test.to_numpy()

# Concatenate to apply sliding window across the boundary
X_w = np.concatenate((X_train_np, X_test_np))
y_w = np.concatenate((y_train, y_test))

# Apply windowing
X_w, y_w = window_data(X_w, y_w, window=WINDOW_LENGTH)

# Split back into train and test (adjusted for windowing)
X_train_w = X_w[:-len(X_test)]
y_train_w = y_w[:-len(X_test)]
X_test_w = X_w[-len(X_test):]
y_test_w = y_w[-len(X_test):]

# Check test set consistency (on log scale)
print(f"Test set equal (log): {np.array_equal(y_test_w, y_test[-len(y_test_w):])}")

# Create TensorFlow datasets
train_data = tf.data.Dataset.from_tensor_slices((X_train_w, y_train_w))
train_data = train_data.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()

val_data = tf.data.Dataset.from_tensor_slices((X_test_w, y_test_w))
val_data = val_data.batch(BATCH_SIZE).repeat()

# Improved model architecture: Stacked bidirectional LSTMs, dropout, ReLU activations
dropout = 0.2
improved_lstm_model = tf.keras.models.Sequential([
    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences=True, input_shape=X_train_w.shape[-2:], dropout=dropout)),
    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, dropout=dropout)),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dropout(dropout),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dropout(dropout),
    tf.keras.layers.Dense(1)
])

# Compile with Adam optimizer and MSE loss (better for log-transformed targets)
improved_lstm_model.compile(optimizer='adam', loss='mse')

# Callbacks for early stopping
early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=1)

# Optional TensorBoard callback
# logdir = "logs/scalars/" + datetime.now().strftime("%Y%m%d-%H%M%S")
# tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)

EVALUATION_INTERVAL = len(X_train_w) // BATCH_SIZE if len(X_train_w) // BATCH_SIZE > 0 else 1
VALIDATION_INTERVAL = len(X_test_w) // BATCH_SIZE if len(X_test_w) // BATCH_SIZE > 0 else 1
EPOCHS = 50  # Increased max epochs, early stopping will handle

model_history = improved_lstm_model.fit(train_data, epochs=EPOCHS,
                                        steps_per_epoch=EVALUATION_INTERVAL,
                                        validation_data=val_data, validation_steps=VALIDATION_INTERVAL,
                                        callbacks=[early_stopping])  # Add tensorboard_callback if needed

# Predict on test (log scale)
yhat_log = improved_lstm_model.predict(X_test_w).reshape(-1)

# Compute smearing factor using train predictions (align with XGBoost)
y_train_pred_log = improved_lstm_model.predict(X_train_w).reshape(-1)
smear_lstm = duan_smearing_factor(y_train_w, y_train_pred_log)

# Inverse transform with smearing
yhat = inverse_log1p_with_smearing(yhat_log, smear_lstm)

# True test values (original scale)
y_test_true = np.expm1(y_test_w)

# Evaluate and store
resultsDict['Improved Tensorflow LSTM'] = evaluate(y_test_true, yhat)
predictionsDict['Improved Tensorflow LSTM'] = yhat

# Print metrics to align with XGBoost output
mae_lstm_test = mean_absolute_error(y_test_true, yhat)
r2_lstm_test = r2_score(y_test_true, yhat)
smape_lstm_test = smape(y_test_true, yhat)
print(f"[TEST] Improved LSTM  MAE: {mae_lstm_test:,.2f} | RÂ²: {r2_lstm_test:.4f} | sMAPE: {smape_lstm_test:.2f}%")
```
